---
title: 'Statistical Learning'
author: Yuan Tian
date: '2019-06-10'
slug: statistical-learning
categories: []
tags: []
image:
  caption: ''
  focal_point: ''
output: 
 blogdown::html_page:
  number_sections: true
  toc: true
  toc_depth: 2
  css: "/css/custom.css"
---

# Little Statistical Learning Book

## Model Representation: a function $h : X ‚Üí Y$

This function $h$ is called a ***hypothesis***. 

Accuracy of our hypothesis function $h$ is measured using a ***cost/loss function***. One particular choice of the loss function for **linear regression** is called "Squared error function" (or "Mean squared error").  

$$J(\theta_0, \theta_1) = \dfrac {1}{2m} \displaystyle \sum _{i=1}^m \left ( \hat{y}_{i}- y_{i} \right)^2 = \dfrac {1}{2m} \displaystyle \sum _{i=1}^m \left (h_\theta (x_{i}) - y_{i} \right)^2$$

The mean is halved (1/2) to simplify the computation of the gradient descent, as the derivative term of the square function will cancel out the (1/2) term. 

Why squared loss (but not absolute loss)?  

> The absolute value is not convenient, because it doesn‚Äôt have a continuous derivative, which makes the function not smooth. Functions that are not smooth create unnecessary difficulties when employing linear algebra to find closed form solutions to optimization problems. Closed form solutions to finding an optimum of a function are simple algebraic expressions and are often preferable to using complex numerical optimization methods, such as gradient descent (used, among others, to train neural networks). --[The 100-page ML Book](http://themlbook.com/)


![](https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/R2YF5Lj3EeajLxLfjQiSjg_110c901f58043f995a35b31431935290_Screen-Shot-2016-12-02-at-5.23.31-PM.png?expiry=1560643200000&hmac=s6t6cOn9_Fsy21OWELBBua4X19cicFE0OWO2RO1nhVE)


## Linear Algebra Review

### What is the [*Inverse* of a Matrix](https://www.mathsisfun.com/algebra/matrix-inverse.html)? 

To have an inverse, the matrix must be "square" ($N_{row} = N_{col}$). 

Inverse of a value. 
![Inverse of a value](https://www.mathsisfun.com/numbers/images/reciprocal-reciprocal.svg)
Inverse of a matrix.
![Inverse of a Matrix](https://www.mathsisfun.com/algebra/images/matrix-inverse-both.svg)
* When we multiply a number by its reciprocal we get 1. 
* When we multiply a matrix by its inverse we get the Identity Matrix. $A \times A^{-1}=A^{-1} \times A = I$

for a $2 \times 2$ matrix, the inverse is: 

![](https://www.mathsisfun.com/algebra/images/matrix-inverse-2x2.svg)

According to the [invertible matrix theorem](http://mathworld.wolfram.com/MatrixInverse.html), The inverse might not exsit, if the [determinant](https://www.mathsisfun.com/algebra/matrix-determinant.html) is zero, such a matrix is called "Singular". 

# ML Resources

## **Superb** Online Courses and Books

* **Best FREE Book**: An Introduction to Statistical Learning in R
  + G. James et al., [An Introduction to Statistical Learning: with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/), Springer Texts in Statistics, DOI 10.1007/978-1-4614-7138-71.
  + [Slides and videos](https://www.r-bloggers.com/in-depth-introduction-to-machine-learning-in-15-hours-of-expert-videos/) for Statistical Learning MOOC by Hastie and Tibshirani
  + [Slides and video](https://www.alsharif.info/#!iom530/c21o7) tutorials related to this book by Abass Al Sharif 

* **Best Coursera Course**: Machine Learning (Stanford University)
  + Approx. 55 hours to complete, course can be found [here](https://www.coursera.org/learn/machine-learning/home/welcome). 

* **Best LITTLE Book**: The hundred-page Machine Learning Book.
  + On average, people spend a week of after-work reading.
  + [Book Wiki](http://themlbook.com/wiki/doku.php)

## Other Resources 

### Data Science Bootcamp June 10-21, 2019

#### R Packages Installations
```{r eval=FALSE}
#Packages for data science: Statistical analysis for high dimensional data
install.packages('e1071')
# Multiclass Logistic Regression
install.packages("glmnet")
install.packages(c("lar","RandomForest","rpart","SIS","tilting"))
#Packages for data science: survival analysis case study
install.packages(c("survival","mstate","p3state.msm","msm"))
```
#### Day 1-2 and 4-5: Machine Learning Statistical Learning

Slides and Resources are [here](https://drive.google.com/drive/folders/1u8N6ISovKj680Yr3Sv5nUeIUFVaZKBK2?usp=sharing). 

#### Day 3: Case Study: MGUS Data (Survival Analyses)

Slides and Resouces are [here](https://drive.google.com/drive/folders/1kBWPsQ-fQJ1KbQw1OtOtlC69DEj_PuCm?usp=sharing)

* **Outcome/Response variable**: time to occurance of an event. 
* **Covariates**: age, sex, pstat and mspike. 
  + mspike: size of the monoclonal serum spike
  + ptime: time until progression to a plasma cell malignancy (PCM) or last contact (months)
  + pstat: occurrence of PCM: 0 = no, 1 = yes
  + futime: time until death or last contact (months)
  + death: occurrence of death: 0 = no, 1 = yes
* **Model**: A multi-state model to describe the path to death. 
  + T: survival time 
  + C: censoring time
  + *t = min(T,C)*
  + $\delta$ = 1 (T $\le$ C) 
    + t is an observed lifetime (full information) if T$\le$ C
    + t is an sensoring time (incomplete information, e.g., withdraw, alive when the study ends) if T$\ge$ C
  + If $h_0(t)$ is left unspecified, then it is called the Cox PH (Proportionality Assumption and Hazard Ratio) Model.

* **How to Interpret HR?** (Example 1):
  + Event: Death
  + Covariates: age, sex, pstat, mspike
  + ùíô = (age = 75, sex, pstat, mspike)‚Ä≤
  + ùíô‚àó= (age = 70, sex, pstat, mspike)‚Ä≤
  + HR = ?

$$HR=e^{\beta_1(75-70) + \beta_2(Sex-Sex) +\beta_3(pstat-pstat) + \beta_4(mspike-mspike) }$$

Given $\beta_1 = 0.05$, $HR=exp(5*0.06)=1.35$. The event relative risk will increase 35% for 5 units (year) controlling for other factors. 

Even though $h_0(t)$ is unspecified, we estimate the $\beta$s. We can estimate $S(t,x)$ using a minimum of assumptions. There are two techniques to adjust the partical likelihood for tied lifetimes: Brceslow and Efron. 

The code can be found in the [Google Drive folder](https://drive.google.com/drive/folders/1u8N6ISovKj680Yr3Sv5nUeIUFVaZKBK2?usp=sharing). 
```{r echo=TRUE}
library(survival)
mgus.data<-read.csv("C:/Users/ytian/Google Drive/PhD/Data Science Bootcamp June 10-21 2019/D3 Code/mgus.data.csv",header = TRUE)
```

#### Day 6-7 Case Study: Diabetes Readmission

Slides and RMarkdown Code can be found [here](https://drive.google.com/drive/folders/1x1UdOzKMrVC4GRqkHeglJhjAwlMDKDk0?usp=sharing). 

```{r eval=FALSE}
install.packages(c("here","olsrr","modelr","broom","caret","neuralnet","DescTools","PredictABEL"))
```

```{r eval=FALSE}
library(magrittr)
library(here)
library(olsrr)
library(modelr)
library(neuralnet)
library(dplyr)
library(PredictABEL)
library(ggplot2)
library(caret)
library(ggplot2)
library(ROCR)
library(broom)
library(DescTools)
```
```{r}
diabetes<-read.csv("C:/Users/ytian/Google Drive/PhD/Data Science Bootcamp June 10-21 2019/C3 Case Study_Diabetes/diabetes_data_full.csv")

diabetes_data <- read.csv("C:/Users/ytian/Google Drive/PhD/Data Science Bootcamp June 10-21 2019/C3 Case Study_Diabetes/diabetes_analytic_data.csv")
```


