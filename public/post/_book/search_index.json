[
["proc-procedure.html", "Little SAS Programming Notes 1 Proc Procedure 1.1 Using PROC SORT to remove duplicates 1.2 Input() and put() for variable type conversion", " Little SAS Programming Notes Yuan Tian 2019-05-09 1 Proc Procedure 1.1 Using PROC SORT to remove duplicates There are three options that might be helpful: DUPOUT=, NODUPRECS, and NODUPKEYS.Code example are from this article: NODUPRECS option identifies observations with identical values for all columns. PROC SORT DATA=Movies OUT=Movies_Sorted_without_DupRecs NODUPRECS ; BY Title ; RUN ; NODUPKEYS (or NODUPKEY) option with PROC SORT removes observations with duplicate keys. In the example below, the duplicate keys data for the column, Title, are removed. It is usualy safer to use NODUPKEYS by specifying the unique keys to sort the data and then remove duplicated observations. PROC SORT DATA=Movies DUPOUT=Movies_Sorted_Dupout_NoDupkey NODUPKEY; BY Title; RUN ; 1.2 Input() and put() for variable type conversion input(char,4.) or input(char,datatime20.12) : Char -&gt; Numeric(/Char) put(numeric,$4.) or put(numeric, datetime19.) : Numeric(/Char) -&gt; Char "],
["data-steps.html", "2 Data Steps 2.1 Some paragraph ", " 2 Data Steps Reference: R for Data Science: Import, Tidy, Transform, Visualize, and Model Data Using R Markdown together with exercises in each chapter of this book, I am trying to document my learning path on R. ‚ÄúHow to get started with R Markdown‚Äù, and the R Markdown Cheatsheet are good reference sources for R Markdown. This article will have tons of examples regarding what R can do for Data Science. Here is my post about SAS programming documentation. This is a piece of code in a block 2.1 Some paragraph this is code "],
["my-little-r-book.html", "3 My Little R Book 3.1 R Packages for R Installation or Update 3.2 Create a Website Using RMarkdown", " 3 My Little R Book 3.1 R Packages for R Installation or Update For new installation, here is a list of most-used packages to install: install.packages(&#39;rmarkdown&#39;) install.packages(&quot;tinytex&quot;) # install TinyTeX (must-do-this-step) tinytex::install_tinytex() install.packages(&#39;tidyverse&#39;) install.packages(&#39;ggplot2&#39;) #table creation with specific formatting install.packages(&#39;kableExtra&#39;) #for writing reproducible research paper in R Markdown install.packages(c(&quot;stargazer&quot;,&quot;plotly&quot;,&quot;knitr&quot;,&quot;bookdown&quot;)) Once in a while, update all the packages by calling update.packages(). 3.2 Create a Website Using RMarkdown This website is created and powered by Hugo, Academic theme, and Netlify. The contents are created using RMarkdown with guidance from the Step-by-Step Instruction provided by ALISON PRESMANES HILL, ‚ÄúBlogdown: Creating Websites with R Markdown‚Äù and R Markdown: The Definitive Guide 3.2.1 The Hugo-Academic Theme A few advantages of the Academic theme include: Latex math rendering via simply enabling it in config toml file. # In config\\_default\\param.toml # Enable global LaTeX math rendering? # If false, you can enable it locally on a per page basis. math = true rich widget system for customization. 3.2.2 Blogdown &amp; Hugo - Fix ToC (with Rmd) The solution to fix Table of Contents (TOC) came from both Create Websites with RMarkdown and Fubits@GIT . In the yaml header of your Rmd file, specific the following: ----- output: blogdown::html_page: number_sections: true toc: true toc_depth: 3 css: &quot;/css/custom.css&quot; ----- Create and save the custom.css under the \\static\\css in the root directory. Using the css formating in link, Your custom.css should look like: /* Add some Heading / Title before the TOC */ #TOC:before{ content: &quot;Table of Content&quot;; font-family: &#39;Lato&#39;, sans-serif!important; font-weight:400; font-size: 26px; } /* Numbering suffix in TOC */ .toc-section-number:after{ content: &quot;. &quot; } /* Numbering suffix in Body/Content */ .header-section-number:after{ content: &quot;. &quot; } "],
["sas-faq-and-tips.html", "4 SAS FAQ and Tips 4.1 Using PROC SORT to remove duplicates 4.2 Input() and put() for variable type conversion 4.3 Merging/Stacking Datasets ‚Äì Truncated Values", " 4 SAS FAQ and Tips The aim is to document code chunks that are likely to be re-used for fast searching and indexing. 4.1 Using PROC SORT to remove duplicates There are three options that might be helpful: DUPOUT=, NODUPRECS, and NODUPKEYS.Code example are from this article: [Recommended] NODUPKEYS (or NODUPKEY) option with PROC SORT removes observations with duplicate keys. Specify the keys, that uniquely identify a observation, in the by statement. In the example below, variable title uniquely identifies a movie. PROC SORT DATA=Movies DUPOUT=Movies_Sorted_Dupout_NoDupkey NODUPKEY; BY Title; RUN ; NODUPRECS option identifies observations with identical values for all columns. PROC SORT DATA=Movies OUT=Movies_Sorted_without_DupRecs NODUPRECS ; BY Title ; RUN ; 4.2 Input() and put() for variable type conversion input(char,4.) or input(char,datatime20.12) : Char -&gt; Numeric(/Char) put(numeric,$4.) or put(numeric, datetime19.) : Numeric(/Char) -&gt; Char 4.3 Merging/Stacking Datasets ‚Äì Truncated Values Stacking multiple datasets into 1 dataset with variables in different length can be tricky. Here is the solution to resolve it. You need to: define the length before set statement; add format _character_. data stacked_ds; length id $20 age 8 comment $200 ; set ds1-ds5; format _character_ ; run; "],
["core-git-commends-and-workflow-in-git-shell.html", "5 Core Git Commends and Workflow in Git Shell 5.1 Two basic commands: git pull and git push. 5.2 4-step common workflow in Git", " 5 Core Git Commends and Workflow in Git Shell Although you can complete most of the git commands via Git Desktop‚Äôs GUI without interfering with the Shell), it is often beneficial to know a few key Git commands and core workflow in the Git Shell. 5.1 Two basic commands: git pull and git push. The git pull can be divided into git fetch and git merge. 5.2 4-step common workflow in Git Assuming you already have the repo on your local machine, then a common workflow will have 4 steps: You ALWAYS need to be sure your local repo is up-to-date with the remote repo. #Make sure there are no untracked files. git status #Pull in all the files changed on the remote git fetch #Comfirm that all the files are ready to merge git status #A final step in pulling the remote changes into local machine git merge #A final &quot;git status&quot; to confirm everything is good to go git status With some modifications/changes in your working directory, it is time to inform Git about your new changes using git add -A. #Stage all the changed or newly added files in the working directory git add -A #Confirm again. git status It‚Äôs time to git commit -m! git commit -m &quot;this is my commit&quot; #Confirm again git status The final git push to the remote repository. git push # Confirm everything is up-to-date git status Other commands that might be useful include: git branch --list, git help, etc. More can be found in the Guide to Git Shell with illustrative figures and tutorials. "],
["day-1-2-machine-learning.html", "6 Day 1-2: Machine Learning 6.1 R Packages Installations 6.2 Feature Vector 6.3 Model Selection", " 6 Day 1-2: Machine Learning 6.1 R Packages Installations #Packages for data science: Statistical analysis for high dimensional data install.packages(&#39;e1071&#39;) # Multiclass Logistic Regression install.packages(&quot;glmnet&quot;) install.packages(c(&quot;lar&quot;,&quot;RandomForest&quot;,&quot;rpart&quot;,&quot;SIS&quot;,&quot;tilting&quot;)) #Packages for data science: survival analysis case study install.packages(c(&quot;survival&quot;,&quot;mstate&quot;,&quot;p3state.msm&quot;,&quot;msm&quot;)) 6.2 Feature Vector 6.3 Model Selection Fill all possible models and select a single best model from according certain criteria (e.g., AIC statistics). AIC statistics ‚Äì a balance between the maximum likelihood and the number covariates (panalizing larger models). BIC statistics ‚Äì very similar to AIC, but with log(n). Stepwise model selection (traditional) backward elimination forward selection Redge Regression (traditional regression + a shrinkage penalty) The LASSO Ridge regression, unlike subset selection, will generally select models that involve just a subset of the variables, while Redge regression will include all p predictors in the final model. "],
["day-3-case-study-mgus-data.html", "7 Day 3: Case Study: MGUS Data 7.1 MGUS Data Study", " 7 Day 3: Case Study: MGUS Data 7.1 MGUS Data Study Outcome/Response variable: time to occurance of an event. Covariates: age, sex, pstat and mspike. mspike: size of the monoclonal serum spike ptime: time until progression to a plasma cell malignancy (PCM) or last contact (months) pstat: occurrence of PCM: 0 = no, 1 = yes futime: time until death or last contact (months) death: occurrence of death: 0 = no, 1 = yes Model: A multi-state model to describe the path to death. T: survival time C: censoring time t = min(T,C) \\(\\delta\\) = 1 (T \\(\\le\\) C) t is an observed lifetime (full information) if T\\(\\le\\) C t is an sensoring time (incomplete information, e.g., withdraw, alive when the study ends) if T\\(\\ge\\) C If \\(h_0(t)\\) is left unspecified, then it is called the Cox PH (Proportionality Assumption and Hazard Ratio) Model. How to Interpret HR? (Example 1): Event: Death Covariates: age, sex, pstat, mspike ùíô = (age = 75, sex, pstat, mspike)‚Ä≤ ùíô‚àó= (age = 70, sex, pstat, mspike)‚Ä≤ HR = ? Answer: \\[HR=e^{\\beta_1(75-70) + \\beta_2(Sex-Sex) +\\beta_3(pstat-pstat) + \\beta_4(mspike-mspike) }\\] \\(HR=e^{\\beta_1(75-70) + \\beta_2(Sex-Sex) +\\beta_3(pstat-pstat) + \\beta_4(mspike-mspike) }\\) \\(\\beta_1 = 0.05\\), \\(HR=exp(5*0.06)=1.35\\) The event relative risk will increase 35% for 5 units (year) controlling for other factors. Page 18. Barry: At that time, all four subjects were at risk for the event. The first term has the sum of the four subjects‚Äô hazards inthe demoniator and Barry‚Äôs hazard in the numberator. Page 22. The idea behind *Checking PH Assumption\" is that if PH assumption holds for a particular covariate, then the Schoenfeld residuals for that covariate will not be related to survival time. Page 23. Why the Cox PH model is popular? The Cox PH model is a robust model, so that results from using the Cox model will closely approximate the results from the correct parametric model. Even though \\(h_0(t)\\) is unspecified, we estimate the \\(\\beta\\)s. We can estimate \\(S(t,x)\\) using a minimum of assumptions. There are two techniques to adjust the partical likelihood for tied lifetimes: Brceslow and Efron. library(survival) mgus.data&lt;-read.csv(&quot;C:/Users/ytian/Google Drive/PhD/Data Science Bootcamp June 10-21 2019/D3 Code/mgus.data.csv&quot;,header = TRUE) with(mgus.data,table(sex)) ## sex ## F M ## 631 753 with(mgus.data,mean(age)) ## [1] 70.42341 with(mgus.data,tapply(age,sex,mean)) ## F M ## 71.32171 69.67065 ########################################## # Standard survival analysis. The event of interest is death. ########################################## sfit0&lt;-coxph(Surv(futime,death)~age+sex+pstat+mspike,mgus.data) summary(sfit0) ## Call: ## coxph(formula = Surv(futime, death) ~ age + sex + pstat + mspike, ## data = mgus.data) ## ## n= 1373, number of events= 957 ## (11 observations deleted due to missingness) ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## age 0.063278 1.065322 0.003462 18.275 &lt; 2e-16 *** ## sexM 0.354309 1.425195 0.065843 5.381 7.4e-08 *** ## pstat 0.303788 1.354982 0.107278 2.832 0.00463 ** ## mspike 0.002737 1.002741 0.060613 0.045 0.96399 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## age 1.065 0.9387 1.0581 1.073 ## sexM 1.425 0.7017 1.2526 1.622 ## pstat 1.355 0.7380 1.0980 1.672 ## mspike 1.003 0.9973 0.8904 1.129 ## ## Concordance= 0.668 (se = 0.01 ) ## Likelihood ratio test= 400.2 on 4 df, p=&lt;2e-16 ## Wald test = 344.7 on 4 df, p=&lt;2e-16 ## Score (logrank) test = 344.4 on 4 df, p=&lt;2e-16 # Analysis by taking pstat time-dependent. # First, we will have to convert the data into counting process format. # What is a counting process format? # Each row of the data set represents a time interval [start, stop). # Covariate values for that row are the covariate values that apply # over that interval. # The event variable for each row is 1 if the time interval ends # with an event and 0 otherwise. newmgus&lt;-tmerge(data1=mgus.data,data2=mgus.data, id=id,status=event(futime,death)) newmgus[1:100,] newmgus1&lt;-tmerge(newmgus,mgus.data,id=id,pstat.td=tdc(ptime)) newmgus1[1:250,] sfit1&lt;-coxph(Surv(tstart,tstop,status)~age+sex+pstat.td+mspike,newmgus1) summary(sfit1) ## Call: ## coxph(formula = Surv(tstart, tstop, status) ~ age + sex + pstat.td + ## mspike, data = newmgus1) ## ## n= 1488, number of events= 957 ## (11 observations deleted due to missingness) ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## age 0.063787 1.065865 0.003509 18.175 &lt; 2e-16 *** ## sexM 0.353876 1.424579 0.065811 5.377 7.57e-08 *** ## pstat.td 1.739385 5.693839 0.108870 15.977 &lt; 2e-16 *** ## mspike -0.064129 0.937884 0.060620 -1.058 0.29 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## age 1.0659 0.9382 1.0586 1.073 ## sexM 1.4246 0.7020 1.2522 1.621 ## pstat.td 5.6938 0.1756 4.5998 7.048 ## mspike 0.9379 1.0662 0.8328 1.056 ## ## Concordance= 0.689 (se = 0.01 ) ## Likelihood ratio test= 566.5 on 4 df, p=&lt;2e-16 ## Wald test = 566.5 on 4 df, p=&lt;2e-16 ## Score (logrank) test = 641.7 on 4 df, p=&lt;2e-16 # Compare the effects of pstat. An incorrect analysis can lead # to misleading conclusion. # BUT, how about the PH assumption? cox.zph(sfit1) ## rho chisq p ## age 0.14380 24.3233 8.14e-07 ## sexM -0.02634 0.6599 4.17e-01 ## pstat.td 0.00478 0.0215 8.83e-01 ## mspike 0.01685 0.3055 5.80e-01 ## GLOBAL NA 26.4304 2.59e-05 # Use survSplit() to split each record into multiple subrecords using a cut time. # Then, fit stratified Cox model. newmgus2&lt;-survSplit(Surv(tstart,tstop,status) ~ ., data= newmgus1, cut=c(60), episode= &quot;tgroup&quot;, id=&quot;id1&quot;) newmgus2[1:200,] newdata&lt;-with(newmgus2,expand.grid(age=70,sex=c(&quot;F&quot;, &quot;M&quot;), pstat.td=c(0,1),mspike=1.2,tgroup=1)) newdata ## age sex pstat.td mspike tgroup ## 1 70 F 0 1.2 1 ## 2 70 M 0 1.2 1 ## 3 70 F 1 1.2 1 ## 4 70 M 1 1.2 1 sfit2&lt;-coxph(Surv(tstart,tstop,status)~age:strata(tgroup)+sex+pstat.td+mspike,newmgus2) cox.zph(sfit2) ## rho chisq p ## sexM -0.02862 0.78043 0.377 ## pstat.td -0.00293 0.00805 0.929 ## mspike 0.01798 0.34884 0.555 ## age:strata(tgroup)tgroup=1 0.04450 2.39974 0.121 ## age:strata(tgroup)tgroup=2 0.00280 0.00823 0.928 ## GLOBAL NA 3.74629 0.586 summary(sfit2) ## Call: ## coxph(formula = Surv(tstart, tstop, status) ~ age:strata(tgroup) + ## sex + pstat.td + mspike, data = newmgus2) ## ## n= 2368, number of events= 957 ## (16 observations deleted due to missingness) ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## sexM 0.365105 1.440665 0.065970 5.534 3.12e-08 ## pstat.td 1.770705 5.874992 0.109339 16.195 &lt; 2e-16 ## mspike -0.049354 0.951844 0.060798 -0.812 0.417 ## age:strata(tgroup)tgroup=1 0.048477 1.049671 0.004728 10.253 &lt; 2e-16 ## age:strata(tgroup)tgroup=2 0.080192 1.083495 0.005141 15.598 &lt; 2e-16 ## ## sexM *** ## pstat.td *** ## mspike ## age:strata(tgroup)tgroup=1 *** ## age:strata(tgroup)tgroup=2 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## sexM 1.4407 0.6941 1.2659 1.640 ## pstat.td 5.8750 0.1702 4.7417 7.279 ## mspike 0.9518 1.0506 0.8449 1.072 ## age:strata(tgroup)tgroup=1 1.0497 0.9527 1.0400 1.059 ## age:strata(tgroup)tgroup=2 1.0835 0.9229 1.0726 1.094 ## ## Concordance= 0.69 (se = 0.01 ) ## Likelihood ratio test= 587.3 on 5 df, p=&lt;2e-16 ## Wald test = 579.2 on 5 df, p=&lt;2e-16 ## Score (logrank) test = 653.6 on 5 df, p=&lt;2e-16 plot(survfit(sfit2, newdata=newdata), col=c(1,1,2,2),lty=c(1,2,1,2),xlab=&quot;Months&quot;, ylab=&quot;Survival Probability&quot;) legend(&quot;bottomleft&quot;, legend=c(&quot;F, pstat=0&quot;,&quot;M, pstat=0&quot;,&quot;F, pstat=1&quot;,&quot;M, pstat=1&quot;), col=c(1,1,2,2),lty=c(1,2,1,2)) pred&lt;-survfit(sfit2,newdata=newdata) quantile(pred,0.5) ## $quantile ## 50 ## 1 NA ## 2 NA ## 3 22 ## 4 10 ## ## $lower ## 50 ## 1 NA ## 2 NA ## 3 13 ## 4 7 ## ## $upper ## 50 ## 1 NA ## 2 NA ## 3 31 ## 4 20 "],
["day-4-high-dimensional-data.html", "8 Day 4: High Dimensional Data", " 8 Day 4: High Dimensional Data What is high dimensional data? The number of subjects \\(n\\) is less than the number of predictors \\(p\\). "]
]
